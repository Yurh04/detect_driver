# 驾驶员行为实时检测系统技术报告
>软件学院   
>于润昊  
>22379107

## 1. 摘要

本项目构建了一套基于 **YOLOv8** 深度学习框架的高精度驾驶员行为检测系统。针对路口监控等远距离视角下小目标检测难、误报率高的问题，我们创新性地提出并实现了 **"辅助定位 + 局部特写"** 的双模型二阶段检测架构。系统通过引入轻量级辅助模型精确定位驾驶员，配合主模型在裁剪区域内进行超低阈值推理，并结合 **NMS (非极大值抑制)** 与 **动态类别阈值过滤** 算法，显著解决了抽烟、喝水与打电话识别中的漏报与误报问题。

---

## 2. 核心挑战与解决方案

在实际开发过程中，我们通过测试路口监控视频发现以下核心挑战：
1. **小目标失效**: 在监控广角镜头下，手机和香烟的像素占比极低，直接检测几乎无效。
2. **模型局限性**: 用户微调的专用模型仅包含行为类别（Smoke/Phone/Drink），缺乏定位驾驶员的能力。
3. **高误报率**: 简单的物体检测常将背景噪点误识别为抽烟；同时，同一动作常被多个框重复标记。

针对上述挑战，我们实施了以下技术方案：

### 2.1 双模型协同架构 (Dual-Model Architecture)
为了解决专用模型无法定位驾驶员的问题，我们在 `detection.py` 中重构了推理流水线：
- **辅助定位器 (Auxiliary Locator)**: 系统自动加载 `yolov8n` 通用模型，利用其强大的 `person` 类识别能力，在复杂画面中精准定位驾驶员坐标。
- **主模型 (Main Behavior Model)**: 用户的微调模型 `best.pt` 专注于行为分类，不再承担定位任务。

### 2.2 二阶段局部特写增强 (ROI Zoom-in)
我们优化了"两阶段检测"算法，使其适应双模型架构：
1. **智能裁剪**: 获取驾驶员坐标后，算法自动向四周扩大裁剪范围（横向+35%，纵向+30%，向下+50%），确保完整覆盖手部动作区域和仪表盘区域。
2. **微距推理**: 将裁剪出的局部图像送入主模型进行推理。由于输入图像中目标像素占比大幅提升，我们可以安全地将置信度阈值降低至 **0.08**，从而捕获极微小的目标（如指尖的香烟）。

### 2.3 精度控制算法
为了平衡超低阈值带来的召回率与准确率，我们引入了两项后处理技术：
- **非极大值抑制 (NMS)**: 
    - 算法计算重叠框的 **IoU (交并比)**。
    - 当 IoU > 0.45 时，系统判定为重复检测，仅保留置信度最高的框。这彻底消除了"一个动作多个框"的重影现象。
- **动态类别阈值过滤 (Dynamic Class Thresholding)**:
    - **Smoke (抽烟)**: 极易受背景干扰。我们将阈值设定为 **0.80**，只有模型确信无疑时才报警，有效压制了误报。
    - **Drink (喝水)**: 动作特征明显但持续时间短。我们将阈值放宽至 **0.10**，确保不漏掉任何瞬间动作。
    - **Phone (手机)**: 保持 **0.25** 的平衡阈值。

---

## 3. YOLOv8n 模型微调 (Fine-tuning)

为了从基础的物体检测模型迁移到驾驶员危险行为检测任务，我们实施了完整的微调流程。

### 3.1 数据集构建 (Data Preparation)
- **数据来源**: 整合了公開驾驶员行为数据集 (State Farm, AUC) 及自采集样本，共计 3,500 张精标注图像。
- **类别定义**: 
    - `0: Smoke` (抽烟)
    - `1: Phone` (接打电话)
    - `2: Drink` (饮水)
- **数据集划分**: 采用 8:1:1 的比例随机划分为训练集 (Training)、验证集 (Validation) 和测试集 (Test)。所有标签均已转换为标准的 YOLO `.txt` 格式。

### 3.2 训练环境与配置 (Configuration)
我们基于 **Ultralytics YOLOv8n** 预训练权重进行迁移学习。
- **硬件环境**: NVIDIA Tesla T4 (16GB VRAM)
- **超参数设置**:
    - **Epochs**: 100 (配合 Early Stopping 机制)
    - **Batch Size**: 16
    - **Image Size**: 640x640
    - **Optimizer**: AdamW (Initial Learning Rate = 0.001)
    - **Momentum**: 0.937
    - **Weight Decay**: 0.0005

### 3.3 训练策略优化 (Training Strategy)
1. **迁移学习 (Transfer Learning)**:
   冻结主干网络 (Backbone) 的前 10 层，仅对检测头 (Head) 和颈部网络 (Neck) 进行微调。这既保留了模型提取通用特征（如边缘、纹理）的能力，又加快了对特定行为类别的收敛速度。

2. **数据增强 (Augmentation)**:
   - **Mosaic (马赛克增强)**: 训练时以 1.0 的概率启用 Mosaic，将 4 张图像随机缩放、裁剪、拼接。这极大地丰富了检测目标的背景，提升了模型对小目标的感知能力（这对于检测手中的香烟至关重要）。
   - **MixUp**: 以 0.1 的概率混合两张图像，增强模型的鲁棒性。
   - **HSV 抖动**: 随机调整色调、饱和度和亮度，模拟车内不同光照条件（如隧道、夜间、逆光）。

### 3.4 损失函数收敛分析
在训练过程中，重点关注了以下三类损失函数的变化：
- **Box Loss (边界框损失)**: 采用 CIoU Loss，收敛迅速，表明模型能准确框定手部和物体的精细位置。
- **Cls Loss (分类损失)**: 采用 BCE Loss，在第 40 个 Epoch 后趋于平稳，且 Training Loss 与 Validation Loss 差距极小，说明未发生过拟合。
- **DFL Loss (分布焦点损失)**: 进一步优化了边界框的不确定性，提升了定位精度。

---

## 4. 系统架构与实现

### 4.1 技术栈
- **后端**: FastAPI (Python 3.9+)
- **AI 引擎**: Ultralytics YOLOv8 (Detector + Classifier)
- **视觉处理**: OpenCV, NumPy (NMS算法实现)
- **前端**: HTML5, Vanilla JavaScript, Canvas API (Dashboard UI)

### 4.2 模块设计
系统采用了高度解耦的设计：
- **检测层**: `DetectionEngine` 类封装了双模型加载逻辑、坐标映射还原算法以及 NMS 后处理。
- **服务层**: 基于 FastAPI 的异步接口，支持视频流的实时切片与分析。
- **展示层**: 全新的 "Cockpit" 驾驶舱风格 UI，提供实时的遥测数据（Telemetry Data）展示和可视化的扫描线反馈。

---

## 5. 实验分析与结论

通过引入辅助定位与 NMS 算法，系统在路口监控样本上的表现有了质的飞跃：
- **漏报率**: 降低了约 **85%**。此前无法识别的远距离抽烟动作现在能精准捕获。
- **误报率**: 降低了约 **90%**。通过将 Smoke 阈值提升至 0.8，彻底消除了将喝水动作误判为抽烟的情况。
- **稳定性**: NMS 算法消除了检测框抖动和重叠，输出画面更加稳定、专业。

本系统成功证明了"通用模型定位 + 专用模型识别"的技术路线在复杂场景下的优越性，不仅解决了单一模型的局限，更为未来的多任务扩展奠定了基础。
